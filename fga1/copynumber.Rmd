---
title: "Identifying copy number alteration using CBS"
output:
  pdf_document:
    keep_tex: yes
    fig_caption: true
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F
                      ,cache=T
                      ,results = 'hide'
                      ,eval.after = c('fig.cap','code')
                      )
options(warn = -1)
library(DNAcopy)

```


### Background: Why are we looking at copy number
DNA copy number refers to the abundancy of a small piece of chomosome, and is of both biological and medical interests. Cancer initiation is often associated with accumulated DNA mutation followed by DNA alteration at a larger scale, because of the inefficiency of DNA repairing mechanism. Copy numeval.after = c('fig.cap','code'),ber alteration can also be observed in germline cells, often associated with genetic diseases. Hence, reliable detection of copy number alteration would enable non-invasive cancer diagnosis and screens for genetic diseases. It also serves as an intermediate between chromosome morphology analysis and sequencing analysis, often conducted through microarray.


```{r simulate_data}
'%f%' <- function(x,f) { do.call(what = f,
                                 args = list(x)) }
source('util_copynumber.R')

###### Define proportion of loss/duplication
p_loss = 0.25
p_norm = 0.5
p_dup  = 0.25
p_vct = c(p_loss,p_norm,p_dup)

stopifnot(p_loss + p_norm + p_dup  == 1)

##### Define average length of each state.
L_loss = 20E4
L_norm = 60E4
L_dup  = 20E4
L_vct = c(L_loss, L_norm, L_dup)


#### Initialise probe grid on chromosome
L_chromosome = 200E6  ### 200 Megabp genome
probes = seq(1,L_chromosome, length.out = 30E3) ### 30 Kbp probes ,aka one probe per 6Kbp  
# length(probes)
probeidx <- probes %f% as.integer
probes[1:5]

##### Assign copynumber to probes
system.time({
  cpnum_act = assign_copynumber(probeidx,p_vct,L_vct, guess = 1.0)
})
# cpnum = segEND
log_cpnum_act =log_convert(cpnum_act)
sigma = 0.2
noisy_log_cpnum_act = noisy( log_cpnum_act, sigma = 0.2, relative = F) #### Absolute error is used here
```



```{r model}
source('util_copynumber.R')

set.seed(0)
L_chromosome = 200E6
# portion = .1
portion = 1.
probeidx = seq(1,L_chromosome * portion, length.out = (30E3 * portion) %f% floor) %f% floor
# probeidx

cpnum_act = assign_copynumber(probeidx, p_vct, L_vct, guess = 1.0)
log_cpnum_act =log_convert(cpnum_act)
sigma = 0.2#
  
noisy_log_cpnum_act = noisy( log_cpnum_act, sigma =sigma, relative = F) #### Absolute error is used here

chrome_idx = 1
# idxs = 1:1E4
mdl = log_cpnum_act
model_binary  = mdl == -1
sigmas = 2^(seq(-2,0.8,length.out= 6) ) - 0.05
```

```{r CBS,eval  = T, fig.cap = cap}
cap = "\\label{fig:CBS_1} CBS result for different noise level. Green: raw probe response; Blue: theoretical model; Red: CBS segmentation"

par(mfrow = c(3,2))
# sigmas = c(0.05,0.1,0.2,0.4)
res = vector('list',length(sigmas))
par(mai = rep(0.4,4), omi = rep(0.2,4))

##### For visualisation, take a small piece of 2E3
chrome_idx = 1
idxs = 1:1E3
for (i in 1:length(sigmas)){
  sigma = sigmas[i]
  noisy_log_cpnum_act = noisy( log_cpnum_act, sigma =sigma, relative = F) #### Absolute error is used here
  vct = noisy_log_cpnum_act[idxs]
  df  = data.frame(chromosome = chrome_idx,
                   vct,
                   position = probeidx[idxs])

  #### Run DNACopy for segmentation
  CNA.object <- CNA(cbind(vct),
                    df$chromosome, df$position,
                    data.type="logratio",sampleid=sprintf("simulated sigma=%.3f", sigma))
  smoothed.CNA.object <- smooth.CNA(CNA.object)
  segment.smoothed.CNA.object <- segment(smoothed.CNA.object, verbose=1)
  
  #### Plot it
  plot(segment.smoothed.CNA.object,type="w", ylim = c(-2, 2), xlim = c(0,2000),
       xlab = 'Probe index',
       ylab = 'logR')
  lines(log_cpnum_act[idxs],col='blue')
}
```
```{r CBS__data,eval = F}
##### Generate the actual data and save to 'res'
chrome_idx = 1
idxs = 1:length(log_cpnum_act)
for (i in 1:length(sigmas)){
  sigma = sigmas[i]
  noisy_log_cpnum_act = noisy( log_cpnum_act, sigma =sigma, relative = F) #### Absolute error is used here
  vct = noisy_log_cpnum_act[idxs]
  df  = data.frame(chromosome = chrome_idx,
                   vct,
                   position = probeidx[idxs])
  
  #### Run DNACopy for segmentation
  CNA.object <- CNA(cbind(vct),
                    df$chromosome, df$position,
                    data.type="logratio",sampleid=sprintf("simulated sigma=%.3f", sigma))
  smoothed.CNA.object <- smooth.CNA(CNA.object)
  segment.smoothed.CNA.object <- segment(smoothed.CNA.object, verbose=1)
  res[[i]] <- segment.smoothed.CNA.object
}
norm.res <- res 
save(norm.res, file ='normal.dat')
```

<!-- ```{r} -->
<!-- par(mfrow = c(3,2)) -->
<!-- load('normal.dat') -->
<!-- res = norm.res -->

<!-- par(mai = rep(0.4,4), omi = rep(0.2,4)) -->

<!-- for(cna in res){ -->
<!--   plot(cna) -->
<!-- } -->
<!-- ``` -->
```{r stat_test, eval = F}
source('util_copynumber.R')
# pdf('')
par(mfrow = c(3,2))
stat_vct = c()
# length(fitted)
load('model')
load('normal.dat')
res = norm.res
for (i in 1:length(res)){
  states = res[[i]]$output$seg.mean
  fitted = states[
    flatten(
    idx = idxs,
    ENDs = res[[i]]$segRows$endRow,
    states = states
    )

  ]
  # fitted[is.na(fitted)] = 0
  # mdl = log_cpnum_act[idxs]
  # plot(fitted,mdl,cex = 0.5)
  error.vct = abs(mdl-fitted)
  # count =   hist(error.vct,plot = F      )
  # df = rbind(data.frame(x=error.vct))
  # ggplot(df) + geom_histogram(aes(x=x))
  # plot(count, log = 'y')

  # ?hist
  test = mean(error.vct)
  # test = mean((mdl-fitted)^2)
  # test = cor(fitted,mdl,method = 'pearson')
  stat_vct[i] = test
  # test = t.test(mdl-fitted)
  # test = lm(mdl~fitted)
  # test = lm(fitted~mdl)
  # test  = cov(fitted,mdl)  / var(mdl)
  # test = cor(mdl,fitted,method = 'pearson')
  summary(test) %f% print
  test %f% print
}
par(mfrow = c(1,1))
plot(sigmas, stat_vct,type = 'b',lwd = 1,pch = 1,
     ylab = error.name,
     xlab = "SD of noise",
     ylim = c(0,ref_error * 1.5))
abline(h = ref_error , col = 2)
text(1,ref_error * 1.1, paste0('ref_error = ',round(ref_error,3)), col = 2)
grid()
# length(fitted)
```
```{r}
load('model')
error.name = "mean_absolute_error"
ref_error = mean(abs(mdl))
```

#### Data simulation
The data is simulated by concatenating a sample of segments. The copy-number of the segments follows a multinomial distribution, whereas their length follows Poisson distribution (\cite instruction ). At the end an artificial chromosome of 200Mbp is generated, as represented by 30K equally-spaced probe, giving an inter-probe distance of appx. 6kbp. We experimented with different parameters of $L_{loss}$, $L_{dup}$,$L_{norm}$, $p_{loss}$, $p_{dup}$, $p_{norm}$, and selected a parameter set that generated aberrations that span around $10^2$ probes  so that there is enough signal to perform CBS segmentation, while keeping the "normal" to be the longest and most likely segment type. The parameters read:

$$
\begin{aligned}
L_{loss} = 20\cdot 10^4bp\\
L_{norm} = 60\cdot 10^4bp\\
L_{dup}  = 20\cdot 10^4bp\\
p_{loss} = 0.25 \\
p_{norm}= 0.5\\
p_{dup}  = 0.25\\
\end{aligned}
$$
Each probe is then assigned a copy-number response of 1/2/3 according to its segment. The response is log-transformed into logR and masked with a gaussian noise
$$
\begin{aligned}
logR = \log_{2}(R/2) + \epsilon \\
\epsilon \sim N(0, \sigma^2)
\end{aligned}
$$

#### Segmentation under noise
Segmentation is performed on the observed logR with DNACopy (1) , for different $\sigma$.  For each dataset, we calculated `r error.name` between the fitted response and the theoretical model according to eq(@eqMAE) to evaluate the quality of the fit. As noise becomes more prevalent,  the prediction becomes less reliable (see figure \ref{fig:contam_stat} and \ref{fig:CBS_1}). Note the `r error.name` needs to be compared against a completely null prediction, that is, the expected error if no segment is predictied. (`r paste0(error.name, '=' ,round( ref_error, 3 ))`). For example, at sigma=`r round(tail(sigmas,1),3)`, the MAE appears worse than not predicting any copy number changes. 

(@eqMAE) $$
MAE=\frac{\sum_{probe}{|logR(probe)-logR_M(probe)}|}{N(probe)}
$$
where $logR_M$ denotes the logR from theoretical model, $logR$ denotes that from the observed probe response.

#### Calling copy numbers
In order to define regions with DNA losses, we first attempted a simple thresholding algorithm, where segment means are compared against a fixed threshold, below which the segment is declared to have underwent a "loss" event. To compare the merit of different thresholds, we plotted  Precision-Recall curve for different thresholds under all noise levels. 

<!-- To figure out how to choose a best threshold, we need to define some metrics that measures goodness of our prediction. -->
<!-- We will measure our prediction using specificity (aka true negative rate) -->

<!-- To define positives and negatives, we want to compare the identified "loss" segments to the theoretical losses segments. TP will be defined as segments with a model segment around the similar location with a significant overlap. We define overlap between segments as  -->

To simplify the evaluation, we ignored the segments for now and compute on a per-probe basis. A model is binariesd with "$logR_M(probe) == -1.0$", whereas a prediction is binarised with "$logR(probe) < threshold$", which are then combined to calculate TP,FP,TN,FN, and converted into precision and recall (eq(@eqPR)). The use of PR curve is justified by the imbalanced nature of the sample. 

(@eqPR) $$
\begin{aligned}
Precision &: P = \frac{TP}{TP+FP} \\
Recall &: R = \frac{TP}{TP+FN}
\end{aligned}
$$

$$
\begin{aligned}
TN = N( model = 1) \\ 
\leftrightarrow TN = N( \log_2{(model/2)} = -1) \\ 
TN + FP = N(fitted < threshold)
\end{aligned}
$$


As can be seen from the PR-curve ( figure \ref{fig:eval_thres}), it is possible to achieve P=1 and R=1 for less noisy data, whereas some trade-off is necessary in treating noisy data. The question then becomes how to find the sweet spot without referencing the actual model. This must involves some calculation on the observation side, e.g by looking at the distribution of fitted segment means. 


<!-- The hope lies in that we can somehow predict the P+R from the observation side. -->


```{r data__PR, eval = T}

# for (i in 1:5){
thres2PR <- function(thres, fitted,model){
        fitted_binary = fitted < thres
        model_binary = model == -1
        stats = crossref(fitted_binary, model_binary, logical = F)
      # print((stats$TP + stats$FP) == 0)
        if ((stats$TP + stats$FP) == 0){
          return(NULL)
        }else{
        P = stats$TP /(stats$TP + stats$FP)
        R = stats$TP /(stats$TP + stats$FN)
        return(list(P=P,R=R))
        }
        
    }

# thres = -.5
load('normal.dat')
res = norm.res
mdl =  log_cpnum_act

thres_vct = seq(-.0, -1.5 , length.out =  25 )
xss = vector('list',length = length(res))
yss = vector('list',length = length(res))
# xss = vector('list',length = res)
for (ri in 1:length(res)){
  df = res[[ri]]
  fitted = extract_fitted(df)
  xs = thres_vct *NA
  ys = xs * NA
  for ( ti  in 1:length(thres_vct) ){
    thres = thres_vct[ti]
    out = thres2PR(thres,fitted, mdl)
    if (is.null(out)){
      break
    }
    xs[ti] = out$P
    ys[ti] = out$R
  }
  print(paste(out$P,out$R))
  xss[[ri]]=xs
  yss[[ri]]=ys
}
prelst = do.call(cbind,args = xss)
reclst = do.call(cbind,args = yss)
```



<!-- We start by further visualising the length-signal distribution (see figure ?). -->
```{r DEFINE__seg_thes}

calc_mad <- function(vct){
median(abs(vct - median(vct) ))
}
seg_thres <- function(signal, MIN_LEN=5, Z_MIN=4, seg = 1, SUPER_THRES  = -0.5 , DEBUG = F){  
  if (length(signal) < 20){
    sprintf("[WARNING] Not enough observation (N = %d < 20) to perform clustering reiliably", length(signal))%f%warning
  }
  vct = sort(signal)
  # start = 0
  # vcti = vct[-(1:start)]
  vcti = vct
  x = c()
  
  if (DEBUG){
    xs = c()
    ys = c()
    zs = c()
  }
  outdf = data.frame(
    avg = NULL,
    stdev=NULL,
    N = NULL,
    t.pval=NULL,
    include = NULL
  )

# superthres = -0.5
thres_curr = NA
  for (ii in  1:length(vcti)){
    i  = vcti[ii]
    if (length(x) >= MIN_LEN){
      
      mad = calc_mad(x)
      z = (i-mean(x)) / mad
      p = pnorm(z)

      if (!is.na(z) & seg){
        if (z > Z_MIN | ii == length(vcti)){
          # ?t.test
          # pval = 
          mx = mean(x)
          ##### Simple thresholding the average 
          # include = (mx < superthres)

          ##### A modified one-tail t-test using MAD as an estimator for sample standard deviation.
          ##### This makes p-value more robust w.r.t outlier.
          pval = pt( (mx - SUPER_THRES)/ (mad/sqrt(length(x))),df = length(x) -1, lower = T)
          include = pval < 1E-3
          
          
          ##### By default, R uses sd() to estimate sample stdev.
          # stat = t.test(x, mu = superthres, alternative = 'smaller')
          # pval = stat$p.value
          # paste0(round(mean(x),5),' ',round(mad,5),' ',length(x))%f%print
          # print(pval) 
          # include = 
          outdf = rbind(outdf,list(avg = mean(x) 
                                   ,stdev = mad
                                   ,N = length(x)
                                   ,include = include))
          if (include){
            thres_curr = max(thres_curr, x, na.rm=T)
          }
          #### Reset the sample vector "x"
          x = c()
        }
      }
    }else{
      z = 0
    }
    x  = c(x,i)
    if (DEBUG){
      ys = c(ys,z)
      xs = c(xs,i)
      zs = c(zs,mad)
    }
    # zs = c(zs,sd(x))
    # print(p)
  }
  # if (sum(outdf$include)==0){
  #   thres_curr = NA
  # }
  if (!DEBUG){
  return(thres_curr)
  }else{
  print(outdf)
  print(thres_curr)
  try({
    plot(xs, ylab = 'logR',xlab = 'Segment Index', ylim = c(-1.5,1.5)
         , main = tail( df$data %f% names,1))
    abline(h = -0.5, col= 2,lty = 2)
    abline(h = thres_curr, col= 3,lty = 2)
    par(new = T)
    # plot(ys,axes = F)
    grid()
    # par(new = T)
    # # zs = exp(zs)
    # plot(zs, ylab = '',axes = F,type = 'l')
  })
  return(list(thres_curr,outdf))
  
}

}


```


```{r APPLY__seg_thres}
thres_lst = sapply(res,function(df){
  seg_thres(df$output$seg.mean) 
}) %f%print
# print(thres)
```

```{r DEBUG__seg_thres, fig.cap = cap}
cap = "\\label{fig:DEBUG_thres} Sorted segmented means. Red: threshold = -0.5; Green: threshold by cluster"
df = res[[4]]
par(mfrow= c(3,2))
par(mai = rep(0.3,4))
# act = 

options(error = recover)
thres_lst = vector('numeric',length = length(res))
for (dfi in 1:length(res)){
  df = res[[dfi]]
  print(df$call)
  ENDs = c(0, df$segRows$endRow)
  len_probe = ENDs[-1] - ENDs[-length(ENDs)]
  signal = df$output$seg.mean
  
# thres_lst[dfi]=
  seg_thres(signal,DEBUG = T)
}
```


The distribution of segment means follow a nice multimodal trend (see figure). Intuitively, optimal threshold should be placed at where the value jumps discontinously. This jump is easy to identify manually, and here we apply a simple gaussian/t-test based clustering to demarcate such discontinous jumps. The clustering algorithm works as follows: we start with an empty vector, and add the segment means one by one from the smallest. Once the vector size reach 5, we start to estimate mean and standard deviation (using MAD as estimator) from the sample defined by the vector, so as to estimate the likelihood of the incoming element to belong to this sample. If the incoming value is significantly deviating from the sample (using a Z-score cutoff), then this sample is dumped as a cluster and we start with an empty vector again. As a result, the whole distribution is segmented into multiple homogeneous normal distributions. 


<!-- In order to find the optimal threshold, we performed a simplified form of hierarchial clustering, that is, we segmented the distribution into clusters that are approximated by homogeneous normal distribution (that is, without obvious outliers), so that inter-cluster distance follows a constraint.  -->

<!-- We inspected the sorted segment me -->
<!-- After inspecting the distribution of predicted segment means, we decide to take a outlier-based approach t -->

<!-- We start by sorting the observed probe responses, and start with the smallest 5 values as our starting distribution. We estimated the mean and the standard deviation for this sample (using MAD as estimator). We then process on a per-probe basis -->



<!-- The clusters are then thresholded according to their arithmetic average and clusters with an average < -0.5 is -->

The resultant clusters are then subject to a one-tail t-test with the alternative hypothesis that $\mu < -0.5$, and the cluster is retained only if there is significant evidence to reject the null (P<0.001). In other words, we threshold segment means on a pre-cluster basis to ensure the consistency of thresholding. As shown in the PR-curve (figure \ref{fig:thres_eval}), using the selected threshold we achieved precision > 90% at all tested level of noises. Moreover, in cases where a smooth elbow is present, it is always identified by the cluster-based threshold, achieving the best possible compromise between precision and recall. 

<!-- As compared with simple thresholding at -0.5, the cluster-based always sits on the start of the elbow, and is more conservative but assuring good precision. -->

Unlike result from simple thresholding at -0.5, the cluster-based result always sits on the start of the elbow, does not suffer from the precision reduction due to noise.

For furture reference, this segmentation algorithm requires several parameters: (1) MIN_LEN: is the smallest sample size for a standard deviation to be estimated (2) Z_MIN: is the z-score cutoff to declare the ending of a cluster.(3) SUPER_THRES: is the prior belief of where the cutoff lies. 
<!-- , albeit at the cost of slightly reduced recall ( -->

```{r plot__PR, fig.cap = cap}
cap = "\\label{fig:eval_thres} Evaluation of different threshold. Circle: Thresholded by cluster. Triangle: Naive thresholding (This plot is somehow broken after switching dataset)"
pxs = do.call(cbind,args = xss)
pys = do.call(cbind,args = yss)
thres_lst0 = sapply(res,function(df){
  seg_thres(df$output$seg.mean) 
}) %f%print

par(mfrow = c(2,2))
par(mai = rep(.2, 4),omi = rep(.2, 4))

matplot(pxs,pys,type = 'l'
        ,xlab = 'Recall'
        ,ylab = 'Precision'
        ,ylim = c(0.6,1.01)
       )
# legend(x = 'bottomright',cex = 0.8
#        , legend = sapply( pxs[1,],function(x){ 
#              # as.expression(
#              # bquote(L[good] ==.(x))
#              # )
#          "hi"
#          }
#            )
#        # , legend =  paste0(expression(L[good]),"=",rownames(OUTPUT_M))
#        , col = 1:dim(pxs)[2]
#        , lty = 1:dim(pxs)[2]
#        , lwd = 1)
# matplot(cbind(xss),cbind(yss))
thres_lst = thres_lst0
ptxs = thres_lst*NA
ptys = thres_lst*NA
model_binary = log_cpnum_act==-1
for (ri in 1:length(res)){
  df = res[[ri]]
  thres = thres_lst[ri]
  # thres = thres_vct[ti]
  fitted = extract_fitted(df)
  fitted_binary = fitted< thres
  stats = crossref(fitted_binary, model_binary, logical = F)
    
  P = stats$TP /(stats$TP + stats$FP)
  R = stats$TP /(stats$TP + stats$FN)
  ptxs[ri] = R
  ptys[ri] = P
  print(paste(P,R))
  # xss[[ri]]=xs
  # yss[[ri]]=ys
  # plot(xs,ys,type = 'b')
}
points(ptxs,ptys)

thres_lst = thres_lst0 * 0 - 0.5
ptxs2 = thres_lst*NA
ptys2 = thres_lst*NA
for (ri in 1:length(res)){
  df = res[[ri]]
  thres = thres_lst[ri]
  # thres = thres_vct[ti]
  fitted = extract_fitted(df)
  fitted_binary = fitted< thres
  stats = crossref(fitted_binary, model_binary, logical = F)
    
  P = stats$TP /(stats$TP + stats$FP)
  R = stats$TP /(stats$TP + stats$FN)
  ptxs2[ri] = R
  ptys2[ri] = P
  print(paste(P,R))
  # xss[[ri]]=xs
  # yss[[ri]]=ys
  # plot(xs,ys,type = 'b')
}
points(ptxs2,ptys2, pch = 2)

matplot(thres_vct,pxs + pys,type = 'l'
        ,pch =1
        ,xlab = 'Threshold'
        ,ylab = 'Precision + Recall'
        )
points(thres_lst0,ptxs+ptys)
points(thres_lst,ptxs2+ptys2, pch = 2)

# plot(vct[-1] - vct[-length(vct)])
# plot(vct)
  # ?AxisSecondary
# signal
# median(c(1,2,3))
# len_probe
# print(xs)
# print(ys)
# ENDs
```


### 3. Impact of mixing a normal cell population
We consider a normal cell contamination that attenuates our signal, so that 
(@eq01) $$
\begin{aligned}
R_{cM}(probe) &= c \cdot 2 + (1-c) \cdot R_M(probe) \\
\log_{2}{(R_{cM}(probe)/2)} &= \log_{2}{(c+(1-c)\cdot R_M(probe) / 2)}
\end{aligned}
$$

We plotted $R_{cM}(probe)$ w.r.t. $R_M(probe)$ on a log-log scale, showing a approximately linear trend near R(probe)=2, indicating that the mapped signals should still be separable, albeit less tolerant to noise. In other words, during outlier-based clustering, SUPER_THRES needs to be set dynamically considering the possibility of contamination, rather than fixed at -0.5. 

Firstly, To understand the effect of a normal cell contamination, we plotted noise-MAE curve for 6 contamination level. Indeed, samples with higher contamination are more prone to noise (see figure \ref{fig:contam_stat}).

Secondly, in order to infer this ratio of contamination, we compute the mean absolute error (MAE, also known as the L1-norm) for a range of candidate contamination ratio and select the one with the least MAE. This is done by calculating the average for distances from predicted segment mean to their most probable copy-number given a candidiate contamination ratio ( eq(@eq01) for $R(probe) \in \{1,2,3\}$ ) . It appears that as contamination increases, the algorithm tends to under-estimate the ratio (figure \ref{fig:contam_est}), conceivably due to less segments being available to support the inference. 


For summary, we compared 5 threshold estimating scheme on the PR-curve:
(1) Naive thresholding at -0.5
(2) Naive thresholding using theoretical contamination
(3) Cluster-based thresholding at -0.5
(4) Cluster-based thresholding using theoretical contamintaion
(5) Cluster-based thresholding using guessed contamination

From the summarizing plot, we observe that thresholding using theoretical contamination reasonably improved the result (type2, type4, as compared to type1, type3). However, due to the difficulty in inferring the contamination, it is not always possible to perfrom type5 reliably.

<!-- The result shows that the contamination ratio can be correctly identified given enough samples, which in turn depends on a small contamination (?give an estimate). We compared quality of estimate at different noise level. -->

<!-- (for df in res, calculate guess_contam, -->
<!-- repeat for several c_curr) -->



<!-- Outline: -->
<!-- 1. Compare the output of "no contamination" and "theoretical contamination" -->
<!-- 2. Comment on whether the contamination can be corrcetly estimated at different noise level -->
<!-- 3. comment on the efficacy of the algorithm given contamination has been correctly estimated. -->
<!-- 3. In the case where contmination cannot be estimated, find out why.  -->




<!-- plot(cs,ys) -->


<!-- $$ -->
<!-- {R_{c}(copy\_number) = } -->
<!-- $$ -->
<!-- $$ -->

<!-- \sum_{a}{b} -->
<!-- $$ -->

In other words, the putative "loss" clusters are shifted towards the neutral signal, with their shape invaried. 

```{r contam, eval = T}
#
contam <- function(vct, c){
  vct*(1-c) + 2*c
}
```



```{r, eval = T}

loss_func <- function( signal,expected ){
  # expected = log_convert(1:3)
  mean(apply(abs(outer(signal,expected,FUN = "-")),MARGIN = 1,FUN=min),na.rm = T)
}


guess_contam <- function(signal, N = 30, DEBUG = F, truth = NULL){
  cs = (0:N)/N
  ys = sapply(cs, function(c){
    loss_func(
      signal,
      log_convert(contam(1:3,c))
    )
    
    })
  mi = which.min(ys)
  if (DEBUG){
    plot(cs,ys,axes = F,xlab = 'Contamination ratio', ylab = '')
    axis(1)
    grid()
    
    try({
      abline( v = truth,col = 2 )
      abline( v = cs[which.min(ys)],col = 1, lty = 2)
    })
  }
  if (is.null(mi)){
    c =   NA
    MAE = NA
  }else{
    c =   cs[mi]
    MAE = ys[mi]
  }
  
  list(c = c, MAE = MAE)
}

# signal = extract_fitted(res[[1]]) 
# guess_contam(signal,30, DEBUG = T, truth = 0)
```

```{r, eval = F}
plot(cs,ys, ylab = 'MAE', xlab = 'contamination ratio')
mi = which.min(ys)
cs[mi]
ys[mi]
# signal = extract_fitted(res[[1]]) 
```

```{r CBS2_param, eval = T}
set.seed(0)

sigmas = 2^(seq(-2,0.8,length.out= 6))
# sigmas = c(0.05,0.1,0.2,0.4)
c_lst = seq(0.2,0.8,length.out = 5)
```
```{r CBS2_sim,eval = F}
par(mfrow = c(3,2))
contamin = vector('list', length(c_lst) + 1)  
for (ci in 1:length(c_lst)){
  res = vector('list',length(sigmas))
  par(mai = rep(0.3,4))
  
  c_curr = c_lst[ci]
  signal = log_convert(contam(cpnum_act, c_curr))
  
  for (i in 1:length(sigmas)){
    sigma = sigmas[i]
  noisy_log_cpnum_act = noisy( signal, sigma =sigma, relative = F) #### Absolute error is used here
  
  chrome_idx = 1
  # idxs = 1:1E4
  idxs = 1:length(log_cpnum_act)
  vct = noisy_log_cpnum_act[idxs]
  df  = data.frame(chromosome = chrome_idx,
             # noisy_log_cpnum_act,
             vct,
             position = probeidx[idxs])
  
  CNA.object <- CNA(cbind(vct),
                    df$chromosome, df$position,
                    data.type="logratio",sampleid=sprintf("simulated sigma=%.3f contamin=%.3f", sigma,c_curr))
  smoothed.CNA.object <- smooth.CNA(CNA.object)
  # plot(CNA.object, plot.type="w")
  
  # smooth.CNA
  # vct%f%length
  nrow(df)
  # df
  ##################################################
  ### code chunk number 5: DNAcopy.Rnw:105-106
  ###################################################
  segment.smoothed.CNA.object <- segment(smoothed.CNA.object, verbose=1)
  res[[i]] <- segment.smoothed.CNA.object
  # plot(segment.smoothed.CNA.object, plot.type="w", ylim = c(-2, 2 ))
  # lines(log_cpnum_act[idxs],col='blue')
  }
  contamin[[ci + 1]]$res <- res
}
contamin[[1]]$res = norm.res
# Ps = vector('list',length = res)
save(contamin, file = 'contamin.dat')
# ?segment

```

```{r stat_test2,eval = F}
# library(ggplot2)
source('util_copynumber.R')
load('contamin.dat')
# par(mfrow = c(1,1))
# jpeg('contam.jpg')
pdf("contam.pdf", width=6, height=4)
par(mfrow = c(1,2))


xs = (5:50)/10
pxs = c(1,2,3)
c = 0.9
ys = (xs*(1-c) + 2*c)
yss = c()
pyss = c()
N = 5
cs = (0:N)/N

stat_vcts = vector('list',length = length(c_lst) + 1)
for (c in c(0,c_lst) ){
  ys = (xs*(1-c) + 2*c)
  yss = rbind(yss,ys)
  pys = (pxs*(1-c) + 2*c)
  pyss = rbind(pyss, pys)  
}
xss = matrix(rep(xs,nrow(yss)),nrow = nrow(yss),byrow = T)
pxss = matrix(rep(pxs,nrow(pyss)),nrow = nrow(pyss),byrow = T)
matplot(t(xss),t(yss),log='xy',type = 'l',
        xlab = 'R(probe)',
        ylab = 'R_c(probe)')
matplot(t(pxss),t(pyss), type = 'p', pch= 1,add = T)


##########

stat_vct = c()

# res = contam.res
# length(fitted)
plot(0,0,ylim = c(0,0.25),axes = F,
     xlab = '',ylab = '',
     ,xlim = c(0,2))
for ( ci in 1:length(contamin)){
  res = contamin[[ci]]$res
  # res=lst$res
  for (i in 1:length(res)){
    states = res[[i]]$output$seg.mean
    fitted = states[
      flatten(
        idx = idxs,
        ENDs = res[[i]]$segRows$endRow,
        states = states
      )
      
      ]
    # fitted[is.na(fitted)] = 0
    # mdl = log_cpnum_act[idxs]
    # plot(fitted,mdl,cex = 0.5)
    error.vct = abs(mdl-fitted)
    # count =   hist(error.vct,plot = F      )
    # df = rbind(data.frame(x=error.vct))
    # ggplot(df) + geom_histogram(aes(x=x))
    # plot(count, log = 'y')
    
    # ?hist
    test = mean(error.vct)
    # test = mean((mdl-fitted)^2)
    # test = cor(fitted,mdl,method = 'pearson')
    stat_vct[i] = test
    # test = t.test(mdl-fitted)
    # test = lm(mdl~fitted)
    # test = lm(fitted~mdl)
    # test  = cov(fitted,mdl)  / var(mdl)
    # test = cor(mdl,fitted,method = 'pearson')
    # summary(test) %f% print
    # test %f% print
  }
  # length(fitted)
  stat_vcts[[ci]] = stat_vct
  error.name = "mean absolute error"
  # par(mfrow = c(1,1))
  par(new = T)
  plot(sigmas, stat_vct,
       type = 'b',lwd = ci,pch = ci,
       ylab = error.name,
       xlab = "SD of noise",  col = ci
       ,ylim = c(0,0.25),axes = F
       ,xlim = c(0,2))
}
# pstat = do.call(cbind, args = stat_vcts)
# matplot(pstat,type = 'b')
axis(1)
axis(2)
grid()

dev.off()
# plot()
# mdl
# error.vct
# xlab
```

\begin{figure}
\includegraphics[width=6in]{contam.pdf}
\label{fig:contam_stat}
\caption{Higher contamination are more prone to noise}
\end{figure}

```{r contam_est, fig.cap = cap}
cap = "\\label{fig:contam_est} Estimating conatmination level by minimising MAE. Red: Actual contamination ratio (c-ratio) Black: estimated c-ratio"
load('contamin.dat')

par(mfrow = c(3,2))
par(mai = rep(0.2 ,4))
par(omi = rep(0.2 ,4))


###########
ci = 4
res = contamin[[ci]]$res
c_curr = c_lst[ci-1]
for (df in res){
# signal = extract_fitted(df) 
signal = df$output$seg.mean
guess_contam(signal,30, DEBUG = T, truth = c_curr)
par(new = T)
plot(sort(signal),axes = F,type = 'p',col = 3
     ,ylim = c(-1.5,1.5))
axis(2)
title(names(df$data)[3])
}
```

```{r plot__PR2,fig.cap = cap}
cap = "\\label{fig:eval_thres2} Evaluation of different threshold. Coloured by increasing noise level)"

mdl = log_cpnum_act
par(mfrow = c(3,2)
    ,mai = rep(0.2,4)
    ,omi = rep(0.2,4))
###### Plot PR curves for all contamination
for (ci in 1:length(contamin)){
  res = contamin[[ci]]$res
  c_curr = c(0,c_lst)[ci]
thres_vct = seq(-.0, -1.5 , length.out =  25 )
xss = vector('list',length = length(res))
yss = vector('list',length = length(res))
# xss = vector('list',length = res)
# Ps = c()
Pss = vector('list',length = length(res))
Rss = vector('list',length = length(res))
# for (i in 1:length(Pss)){
#   Pss[[i]] = c(0,0,0)
#   Rss[[i]] = c(0,0,0)
# }
# Rs = c()
# Ps = c()
# Rs = c()
for (ri in 1:length(res)){
  df = res[[ri]]
  fitted = extract_fitted(df)
  xs = thres_vct *NA
  ys = xs * NA
  for ( ti  in 1:length(thres_vct) ){
    thres = thres_vct[ti]
    out = thres2PR( thres, fitted, mdl)
    if (is.null(out)){
      break
    }
    xs[ti] = out$P
    ys[ti] = out$R
    

  }
  
  print(paste(out$P,out$R))
  xss[[ri]]=xs
  yss[[ri]]=ys
  
  thres = -0.5
  thres_contam = log_convert(contam(1,c_curr)) / 2
  thres_contam_guessed = guess_contam(df$output$seg.mean, N = 30)$c
  print(paste0("guessed threshold", thres_contam_guessed))
  
  ###### using -0.5 threshold
  out = thres2PR(thres, fitted, mdl)
  if (is.null(out)){  out = list(P = 1,R = 0) }
  Pss[[ri]][1] = out$P
  Rss[[ri]][1] = out$R
  
  
  ###### Using theoretical contamination
  out = thres2PR(thres_contam, fitted, mdl)
  if (!is.null(out)){
  Pss[[ri]][2] = out$P
  Rss[[ri]][2] = out$R
  #out = list(P = 1,R = 0) 
  }
  
  ##### Using -0.5 + clustering
  thres_clu = seg_thres(df$output$seg.mean,
            SUPER_THRES = thres
            # ,DEBUG = T)[[1]]
            ,DEBUG = F)
  if(thres_clu %f% is.na){ thres_clu = thres}
  # print(thres)
  out = thres2PR(thres_clu, fitted, mdl)
  if (!is.null(out)){
  Pss[[ri]][3] = out$P
  Rss[[ri]][3] = out$R
  #out = list(P = 1,R = 0) 
  }

  
  
  ##### Using theoretical conatmination + clustering
  thres_clu = seg_thres(df$output$seg.mean,
            SUPER_THRES = thres_contam  ##### Using -0.5 + clustering
            ,DEBUG = F)
  if(thres_clu %f% is.na){ thres_clu = thres_contam}
  out = thres2PR(thres_clu, fitted, mdl)
  if (!is.null(out)){
  Pss[[ri]][4] = out$P
  Rss[[ri]][4] = out$R
  #out = list(P = 1,R = 0) 
  }
  
  # thres_clu = seg_thres(df$output$seg.mean,
  #           SUPER_THRES = thres_contam_guessed
  #           ,DEBUG = F)
  # if(thres_clu %f% is.na){ thres_clu = thres_contam_guessed}
  out = thres2PR(thres_contam_guessed, fitted, mdl)
  if (!is.null(out)){
  Pss[[ri]][5] = out$P
  Rss[[ri]][5] = out$R
  }
  
  
  
  thres_clu = seg_thres(df$output$seg.mean,
            SUPER_THRES = thres_contam_guessed
            ,DEBUG = F)
  if(thres_clu %f% is.na){ thres_clu = thres_contam_guessed}
  out = thres2PR(thres_clu, fitted, mdl)
  if (!is.null(out)){
  Pss[[ri]][6] = out$P
  Rss[[ri]][6] = out$R
  out = list(P = 1,R = 0)
  }

}
prelst = do.call(cbind,args = xss)
reclst = do.call(cbind,args = yss)

matplot(reclst, prelst
        , type = 'l'
        , pch = 1
        ,ylim = c(0.6, 1.01)
        ,main = sprintf("c-ratio = %.3f" , c_curr))
# par(new  = T)
Pss = do.call(cbind,args = Pss)
Rss = do.call(cbind,args = Rss)
# matplot(Rss,Pss, type= 'p',pch = matrix(rep(1:4,6),nrow = 4, byrow = F), add = T)
matplot( t(Rss),jitter(t(Pss),factor = 5 ), type= 'p', add = T)
# par( new = T)
# points(Rs,Ps, pch = 2)

}
# dim(Rss)
### matplot(prelst, reclst)
### 

##### Run with simple thresholding using theoretical contam
### P,R 

##### Run with cluster thresholding using theoretical contam  ##### Using -0.5 + clustering
### P,R

##### Run with cluster thresholding using estimated contam
### P,R

```

#### Conclusion
We have tested the efficacy of CBS algorithm using a simulated dataset and assessed its performance under different noise level. Furthermore, we suggested to use cluster-based thresholding to improve the copy-number calling. We then go on to adapt this methodology under a normal cell contamination by incorporating an estimator for contamination, but uncovering the unfortunate fact that the weakness in contamination estimation is destroying the whole algorithm. However, we highlight the usefulness of both naive thresholding and cluster-based thresholding, given that the contamination ratio. Thus the whole system, will benefit from a robust estimator for contamintation ratio.


```{r}
dim(Pss
  )
 mat=  matrix(rep(1:4,6),nrow = 4, byrow = F)
 dim(mat)
 mat
```
```{r eval = F}

# for (i in 1:5){


# thres = -.5
par(mfrow = c(3,2))
par(mai = rep(.2, 4))

thres_vct = seq(-.0, -1.5 , -.05)
xss = vector('list',length = length(res))
yss = vector('list',length = length(res))
# xss = vector('list',length = res)
for (ri in 1:length(res)){
  df = res[[ri]]
  
  xs = thres_vct *NA
  ys = xs * NA
  for ( ti  in 1:length(thres_vct) ){
    thres = thres_vct[ti]
    fitted = extract_fitted(df)
    fitted_binary = fitted < thres
    stats = crossref(fitted_binary, model_binary, logical = F)
    
  # allNEG = fitted< thres
  # allNEG = fitted > thres
  # TP= (mdl == -1) & allNEG
  # print(sum(TN))
  # print(sum(allNEG))
  if ((stats$TP + stats$FP) == 0){
    break
  }
  P = stats$TP /(stats$TP + stats$FP)
  R = stats$TP /(stats$TP + stats$FN)
  xs[ti] = R
  ys[ti] = P
  # paste0("Specificity=",stats$TN/(stats$TN + stats$FP) )%f% print
  }
  print(paste(P,R))
  xss[[ri]]=xs
  yss[[ri]]=ys
  # plot(xs,ys,type = 'b')
}
# stats
pxs = do.call(cbind,args = xss)
pys = do.call(cbind,args = yss)
```



```{r eval = F}
# pxs = do.call(cbind,args = xss)
# pys = do.call(cbind,args = yss)
thres_lst = sapply(res,function(df){
  seg_thres(df$output$seg.mean) 
}) %f%print

par(mfrow=c(1,2))
matplot(pxs,pys,type = 'l'
        ,xlab = 'Recall'
        ,ylab = 'Precision')

legend(x = 'bottomright',cex = 0.8
       , legend = sapply( pxs[1,],function(x){ 
             # as.expression(
             # bquote(L[good] ==.(x))
             # )
         "hi"
         }
           )
       # , legend =  paste0(expression(L[good]),"=",rownames(OUTPUT_M))
       , col = 1:dim(pxs)[2]
       , lty = 1:dim(pxs)[2]
       , lwd = 1)
# matplot(cbind(xss),cbind(yss))


thres_lst
xs = thres_lst*NA
ys = thres_lst*NA
for (ri in 1:length(res)){
  df = res[[ri]]
  thres = thres_lst[ri]
  # thres = thres_vct[ti]
  fitted = extract_fitted(df)
  fitted_binary = fitted< thres
  stats = crossref(fitted_binary, model_binary, logical = F)
    
  P = stats$TP /(stats$TP + stats$FP)
  R = stats$TP /(stats$TP + stats$FN)
  xs[ri] = R
  ys[ri] = P
  print(paste(P,R))
  # xss[[ri]]=xs
  # yss[[ri]]=ys
  # plot(xs,ys,type = 'b')
}
points(xs,ys,add)


matplot(thres_vct,pxs + pys,type = 'l'
        ,pch =1
        ,xlab = 'Threshold'
        ,ylab = 'Precision + Recall')
points(thres_lst,xs+ys)

# plot(vct[-1] - vct[-length(vct)])
# plot(vct)
  # ?AxisSecondary
# signal
# median(c(1,2,3))
# len_probe
print(xs)
print(ys)
# ENDs
```
```{r, eval = F}
df = res[[4]]
outdf =  seg_thres(df$output$seg.mean
            # SUPER_THRES = thres
            ,DEBUG = T)[[2]]
outdf
```
```{r eval = F}
c_curr = 0.2
N = 30
cs = (0:N) / N
ys = c()

for (c_curr in cs){
  
expected = log_convert(contam(1:3, c_curr))
loglike = function(x){
    pval = max(
      # log(
        outer(x,expected, function(x,expected){
        log(2) + pt( abs(x[1] - expected)/ (x[2]) * sqrt(x[3]),df = x[3] -1, lower = F, log.p = T)
        })
      # )
    ) 
}
loss = apply(outdf,MARGIN = 1
             , FUN = loglike
             # , FUN =  function(x){x[1]}    # include = pval < 1E-3
  ) %f%sum
ys = c(ys,loss)
# print(loss)
}
plot(cs,ys)
# loss
ma = which.max(ys)
ma
cs[ma]

# sapply( t(outdf)%f%as.data.frame, FUN = function(x){x$N})
# t(outdf)
```
```{r eval = F}
# pxs = do.call(cbind,args = xss)
# pys = do.call(cbind,args = yss)
# c
  # out = guess

thres_lst = sapply(res,function(df){
  out = guess_contam(extract_fitted(df), N = 30, DEBUG = F);
  # print(out);
  c_curr = out$c;
  # c_curr = .6;
  thres = log_convert(contam(1,c_curr))/2;
  print(thres)
  
  seg_thres(df$output$seg.mean,
            SUPER_THRES = thres
            # ,DEBUG = T)[[1]]
            ,DEBUG = F)
}) %f%print

par(mfrow=c(1,2))
matplot(pxs,pys,type = 'l'
        ,xlab = 'Recall'
        ,ylab = 'Precision')

legend(x = 'bottomright',cex = 0.8
       , legend = sapply( pxs[1,],function(x){ 
             # as.expression(
             # bquote(L[good] ==.(x))
             # )
         "hi"
         }
           )
       # , legend =  paste0(expression(L[good]),"=",rownames(OUTPUT_M))
       , col = 1:dim(pxs)[2]
       , lty = 1:dim(pxs)[2]
       , lwd = 1)
# matplot(cbind(xss),cbind(yss))


thres_lst
xs = thres_lst*NA
ys = thres_lst*NA
for (ri in 1:length(res)){
  df = res[[ri]]
  thres = thres_lst[ri]
  # thres = thres_vct[ti]
  fitted = extract_fitted(df)
  fitted_binary = fitted< thres
  stats = crossref(fitted_binary, model_binary, logical = F)
    
  P = stats$TP /(stats$TP + stats$FP)
  R = stats$TP /(stats$TP + stats$FN)
  xs[ri] = R
  ys[ri] = P
  print(paste(P,R))
  # xss[[ri]]=xs
  # yss[[ri]]=ys
  # plot(xs,ys,type = 'b')
}
points(xs,ys)


matplot(thres_vct,pxs + pys,type = 'l'
        ,pch =1
        ,xlab = 'Threshold'
        ,ylab = 'Precision + Recall')
points(thres_lst,xs+ys)

# plot(vct[-1] - vct[-length(vct)])
# plot(vct)
  # ?AxisSecondary
# signal
# median(c(1,2,3))
# len_probe
print(xs)
print(ys)
# ENDs
```



```{r eval = F}
thres = -.5
fitted_binary = fitted< thres
plot(fitted_binary,type = 'l')
points(model_binary)
# stats = 


sum(fitted_binary & model_binary)
sum(fitted_binary & !model_binary)
sum(fitted_binary)
# min(fitted)
# plot(fitted,)
```

```{r, eval = F}
# vct = 1:500
length(res)
min(res[[1]]$segRows$endRow)
# flatten(
#   idx = vct,
#   ENDs = res[[1]]$segRows$endRow,
#   states = res[[1]]$output$seg.mean
# )
# sapply(probe_states,length)

# idx
# find_seg(1,ENDs)
# ENDs
# find_seg
probe_values = states[probe_states]
plot(probe_values[1:length(vct)])
points(log_cpnum_act[1:length(vct)],col=2)
mdl = log_cpnum_act[1:length(vct)]
fitted = probe_values

# length(fitted)

test = aov(mdl[-length(mdl)]~fitted)
summary(test)
length(fitted)-length(mdl)
length(fitted)
# mdl
# length(vct)
# flat

# aov()
# plot(segment.smoothed.CNA.object$data$simulated.sigma.2.000)
# segment.smoothed.CNA.object
# res[[1]]$output
```


```{r,eval = F}
set.seed(25)
genomdat <- rnorm(500, sd=0.1) +
rep(c(-0.2,0.1,1,-0.5,0.2,-0.5,0.1,-0.2),c(137,87,17,49,29,52,87,42))
plot(genomdat)
chrom <- rep(1:2,c(290,210))
maploc <- c(1:290,1:210)
test1 <- segment(CNA(genomdat, chrom, maploc))
plot(test1)

set.seed(51)
genomdat <- rnorm(500, sd=0.2) +
rep(c(-0.2,0.1,1,-0.5,0.2,-0.5,0.1,-0.2),c(137,87,17,49,29,52,87,42))
plot(genomdat)
chrom <- rep(1:2,c(290,210))
maploc <- c(1:290,1:210)
test2 <- segment(CNA(genomdat, chrom, maploc))
plot(test2)
```
```{r eval = F}
library(DNAcopy)

data(coriell)
# rm(data)

###################################################
### code chunk number 3: DNAcopy.Rnw:85-88
###################################################
CNA.object <- CNA(cbind(coriell$Coriell.05296),
                  coriell$Chromosome,coriell$Position,
                  data.type="logratio",sampleid="c05296")
smoothed.CNA.object <- smooth.CNA(CNA.object)


###################################################
### code chunk number 5: DNAcopy.Rnw:105-106
###################################################
segment.smoothed.CNA.object <- segment(smoothed.CNA.object, verbose=1)
segment
###################################################
### code chunk number 6: DNAcopy.Rnw:120-121
###################################################
plot(segment.smoothed.CNA.object, plot.type="w")

###################################################
### code chunk number 7: DNAcopy.Rnw:129-130
###################################################
# par(mai = rep(0.05,4))
par(mai = c(.1,.1,.1,.1))
par()$mai
par(mar=c(1,1,1,1))
plot(segment.smoothed.CNA.object, plot.type="s") 


###################################################
### code chunk number 8: DNAcopy.Rnw:157-158
###################################################
plot(segment.smoothed.CNA.object, plot.type="p")


###################################################
### code chunk number 9: DNAcopy.Rnw:169-172
###################################################
sdundo.CNA.object <- segment(smoothed.CNA.object, 
                             undo.splits="sdundo", 
                             undo.SD=3,verbose=1)


###################################################
### code chunk number 10: DNAcopy.Rnw:177-178
###################################################
plot(sdundo.CNA.object,plot.type="s")


```







```{r timer_playground,eval = F}
system.time(
rep(0,1E7)
)
system.time(
{a = 1:1E7}
)
system.time(
{b = a * 0}
)

```





```{r assign_copynumber,eval = F}
# seg_count
# probe_count
# probe_count
# sample_segment <- function(N, p_vct, L_vct, s_vct){
#   seg_states = sample( s_vct, N ,replace = T,prob = p_vct)
#   # rpois( 1:(seg_states%f%length),L_vct[seg_states])
#   segL_vct = rpois( 1 : (seg_states%f%length), L_vct[seg_states])
#   
# }

assign_copynumber <- function(probidx, p_vct, L_vct, L_chromosome = tail(probidx,1), maxIter = 5){
  
  L_avg = p_vct %*% L_vct 
  seg_count = (L_chromosome / L_avg) 
  seg_states = c()
  segEND = c(0)
  # segEND_lst = vector(maxIter,'list')
  for (i in 1:maxIter){
    # sample_segment( N = seg_count * 1.5 %f% floor, p_vct, L_vct, s_vct = c(1,2,3) )
    N = seg_count * 1.5 %f% floor
    s_vct =  c(1,2,3)
    seg_states_new = sample( s_vct, N ,replace = T,prob = p_vct)
    seg_states = c(seg_states, seg_states_new)
    segL_vct =  rpois( 1 : (seg_states%f%length), L_vct[seg_states])
    # segL_vct = sample(c(1,2,3), N ,replace = T,prob = p_vct)
    segEND_new = tail(segEND,1) + cumsum(segL_vct)
    segEND = c(segEND,segEND_new)
    allEND = tail(segEND,1) 
    if  ( L_chromosome  - allEND < 0 ){
      ### Length achieved 
      break
    }else{
      if (i==maxIter){
        #### This should be rare
        stop("[ERROR]: Failed to generate a chromosom of wanted length, try increase 'maxIter' ")
      }else{
        ###  update estimated chromosome
        seg_count = (L_chromosome - allEND) / L_avg
      }
    }
  }
    # segEND[-1]
  # segEND
  find_seg <- function(probe){
    which((probe <= segEND[-1]) & (probe > segEND[-length(segEND)]))
  }
  probe_states=sapply(probes, FUN = find_seg)
  # %f%as.vector
  cpnum = seg_states[probe_states]}

log_convert <- function(vct){
  log2(vct/2)
}

noisy <- function(vct, sigma = .5, relative = F){
  if (relative){
    sigma =  abs(vct) * sigma
  }
  vct + rnorm(vct, mean = 0, sd =sigma)
}
```

```{r eval = F}
# probes%f%is
# 
# system.time({
#   segEND = assign_copynumber(probes,p_vct,L_vct)
# })
# 
# cpnum = segEND
# data = noisy(log_convert(cpnum), sigma = 0.05)
# plot( noisy(log_convert(cpnum)[1:1E5] ,sigma = 0.01))
# plot( noisy(log_convert(cpnum[1:1E3]),sigma = 0.05))
# length(noisy(log_convert(cpnum[1:1E5]),sigma = 0.05))
# + 0
# ?rnorm
```
```{r eval =F }

```

```
tpbs = pbs[1:10]
i = 1
(probes[i] <= tpbs[-1]) & (probes[i] > tpbs[-length(tpbs)]) 

# probes_mat = matrix(probes,ncol = 1)
probe = probes[1]

# copy_number = (probes_mat <= segEND[-1]) & (probes_mat >= segEND[-length(segEND)])
# copy_number%f%dim
# copynumber

```
```{r eval = F}
# c()[-1] | 10
# NULL || 1
```


References:
1. Adam B. Olshen, E. S. Venkatraman, Robert Lucito, Michael Wigler; Circular binary segmentation for the analysis of array based DNA copy number data, Biostatistics, Volume 5, Issue 4, 1 October 2004, Pages 557â€“572,



```{r ASCAT,eval = F}
# library(ASCAT)
source("/local/data/public/RLib/ASCAT/R/ascat.R")

# bins <- getBinAnnotations(binSize=15)

setwd('ExampleData/')
ascat.bc = ascat.loadData("Tumor_LogR.txt","Tumor_BAF.txt","Germline_LogR.txt", "Germline_BAF.txt")
ascat.plotRawData(ascat.bc)
ascat.bc = ascat.aspcf(ascat.bc)
ascat.plotSegmentedData(ascat.bc)
ascat.output = ascat.runAscat(ascat.bc) 
```